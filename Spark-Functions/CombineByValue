SparkCombineByKey takes three function as an argument.
1. createCombiner
2. mergeValue
3. mergeCombiner

--create combiner
* It is a first aggregation step for each key
* It will be executed when any new key is found in a partition
* Execution of this lambda function is local to a partition of a node, on each individual values

--mergeValue function of combineByKey
*  Second function executes when next subsequent value is given to combiner
*  It also executes locally on each partition of a node and combines all values
*  It combines a new value in existing accumulator

--mergeCombiner function of combineByKey
*  Final function is used to combine how to merge two accumulators (i.e. combiners) of a single key across the partitions
   to generate final expected result
*  Arguments are two accumulators (i.e. combiners)
*  Merge results of a single key from different partitions


Important Points
*  Apache spark combineByKey is a transformation operation hence its evaluation is lazy
*  It is a wider operation as it shuffles data in the last stage of aggregation and creates another RDD
*  Recommended to use when you need to do further aggregation on grouped data
*  Use combineByKey when return type differs than source type (i.e. when you cannot use reduceByKey )

----------------------------------------------------------------------------------------------------

scala> val studentRDD = sc.parallelize(Array(
     |     ("Joseph", "Maths", 83), ("Joseph", "Physics", 74), ("Joseph", "Chemistry", 91), 
     |     ("Joseph", "Biology", 82), ("Jimmy", "Maths", 69), ("Jimmy", "Physics", 62), 
     |     ("Jimmy", "Chemistry", 97), ("Jimmy", "Biology", 80), ("Tina", "Maths", 78), 
     |     ("Tina", "Physics", 73), ("Tina", "Chemistry", 68), ("Tina", "Biology", 87), 
     |     ("Thomas", "Maths", 87), ("Thomas", "Physics", 93), ("Thomas", "Chemistry", 91), 
     |     ("Thomas", "Biology", 74), ("Cory", "Maths", 56), ("Cory", "Physics", 65), 
     |     ("Cory", "Chemistry", 71), ("Cory", "Biology", 68), ("Jackeline", "Maths", 86), 
     |     ("Jackeline", "Physics", 62), ("Jackeline", "Chemistry", 75), ("Jackeline", "Biology", 83), 
     |     ("Juan", "Maths", 63), ("Juan", "Physics", 69), ("Juan", "Chemistry", 64), 
     |     ("Juan", "Biology", 60)), 3)
studentRDD: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[15] at parallelize at <console>:24

//Defining createCombiner, mergeValue and mergeCombiner functions
scala> def createCombiner = (tuple: (String, Int)) => 
     |     (tuple._2.toDouble, 1)
createCombiner: ((String, Int)) => (Double, Int)



scala> def mergeValue = (accumulator: (Double, Int), element: (String, Int)) => 
     |     (accumulator._1 + element._2, accumulator._2 + 1)
mergeValue: ((Double, Int), (String, Int)) => (Double, Int)

// use combineByKey for finding percentage
scala> :paste
// Entering paste mode (ctrl-D to finish)
val combRDD = studentRDD.map(t => (t._1, (t._2, t._3)))
                        .combineByKey(createCombiner, mergeValue, mergeCombiner)
                        .map(e => (e._1, e._2._1/e._2._2))
// Exiting paste mode, now interpreting.
combRDD: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[19] at map at <console>:33


scala> combRDD.collect foreach println
(Tina,76.5)
(Thomas,86.25)
(Jackeline,76.5)
(Joseph,82.5)
(Juan,64.0)
(Jimmy,77.0)
(Cory,65.0)
