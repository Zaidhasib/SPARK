----------------Group By In Apache Spark----------------------------

Group By is a data group together which is having the same key. It is a lazy transformation.
This is a wide operation which will result in data shuffling . which will costly. This will work in both unpaired and paired 
RDD's. 

scala> val baseRdd = sc.parallelize(Seq((1, 2), (1, 3), (2, 1), (2, 2), (2, 3)))
baseRdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> baseRdd.groupBy(_._1).collect()
res3: Array[(Int, Iterable[(Int, Int)])] = Array((2,CompactBuffer((2,1), (2,2), (2,3))), (1,CompactBuffer((1,2), (1,3))))


------------Group By Key------------------------------------------

GroupByKey will only work for paired RDD . This operation is also very costly operations. This is based on Hash partition implementation.

scala>  val baseRdd = sc.parallelize(Seq((1, 2), (1, 3), (2, 1),(2,2), (2, 3)))
baseRdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:24


scala> baseRdd.groupByKey().collect()
res2: Array[(Int, Iterable[Int])] = Array((2,CompactBuffer(1, 2, 3)), (1,CompactBuffer(2, 3)))
---------------------------------------------------------------------

--------------------------Difference---------------------------------
GroupByKey is more efficient because of Hash-Partitioning.

